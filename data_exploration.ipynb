{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"lfw-deepfunneled\")\n",
    "train_path = data_path / \"train\"\n",
    "test_path = data_path / \"test\"\n",
    "\n",
    "train_images = list(train_path.glob(\"*/*\"))\n",
    "test_images = list(test_path.glob(\"*/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_one_image(img_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        info for a single image\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    img = np.asarray(img)\n",
    "    shape = img.shape\n",
    "    assert len(shape) < 4, f\"{img_path} has alpha channel\"\n",
    "    H, W, C = shape\n",
    "    assert H == 250, f\"{img_path} height is {H}, expected 250\"\n",
    "    assert W == 250, f\"{img_path} width is {W}, expected 250\"\n",
    "    assert C == 3, f\"{img_path} channels is {C}, expected 3\"\n",
    "\n",
    "    person = img_path.parent.stem\n",
    "    img_identifier = img_path.stem\n",
    "    file_extension = img_path.suffix\n",
    "\n",
    "    assert file_extension == \".jpg\", f\"{img_path} file extension is {file_extension}, expected .jpg\"\n",
    "\n",
    "    data = {\"person\": person,\n",
    "            \"identifier\": img_identifier,\n",
    "            \"file_extension\": file_extension,\n",
    "            \"height\": H,\n",
    "            \"width\": W,\n",
    "            \"channels\": C}\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_info_all_images(paths: list[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        info for all images in paths\n",
    "    \"\"\"\n",
    "    all_data = defaultdict(list)\n",
    "    for p in paths:\n",
    "        data = get_info_one_image(p)\n",
    "        for k, v in data.items():\n",
    "            all_data[k].append(v)\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "train_info_df = get_info_all_images(train_images)\n",
    "test_info_df = get_info_all_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12185\n",
      "5603\n",
      "1534\n"
     ]
    }
   ],
   "source": [
    "# ~12k datapoints, ~5.6k unique people, ~1.5k have more than one person\n",
    "print(len(train_info_df))\n",
    "person_groups = train_info_df.groupby(\"person\")\n",
    "\n",
    "print(len(person_groups))\n",
    "print((person_groups.size() > 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048\n",
      "756\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "# test set is ~1k datapoints, ~750 unique people\n",
    "print(len(test_info_df))\n",
    "person_groups = test_info_df.groupby(\"person\")\n",
    "\n",
    "print(len(person_groups))\n",
    "print((person_groups.size() > 1).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0928ff113c817ceb53dc04aa4488ddd363be84135edfadafdc532f51fd32240d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
